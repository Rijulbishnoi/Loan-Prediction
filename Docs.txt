this is local yaml file --->

name: MLOps CI/CD

on:
  push:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: âœ… Checkout Code
        uses: actions/checkout@v2

      - name: âœ… Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: âœ… Install Dependencies
        run: pip install -r requirements.txt

      - name: âœ… Set Python Path
        run: echo "PYTHONPATH=$PYTHONPATH:$(pwd)" >> $GITHUB_ENV

      - name: âœ… Lint Code
        run: pylint src/*.py api/*.py --disable=R,C

  test:
    needs: build
    runs-on: ubuntu-latest

    steps:
      - name: âœ… Checkout Code
        uses: actions/checkout@v2

      - name: âœ… Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.9

      - name: âœ… Install Dependencies
        run: pip install -r requirements.txt

      - name: âœ… Run Unit Tests
        run: pytest tests/ --disable-warnings

  deploy:
    needs: test
    runs-on: ubuntu-latest

    steps:
      - name: âœ… Checkout Code
        uses: actions/checkout@v2

      - name: âœ… Deploy Application
        run: |
          echo "ğŸš€ Deploying application..."
          # Add deployment script here (e.g., Docker, AWS, Heroku, etc.)


this is on server yaml file ------> 
This is in .github folder .




For Dockerfile --->>>>>

docker build -t saurabhbrd/loan_default_app .
docker run -p 8501:8501 saurabhbrd/loan_default_app
docker login 
docker push saurabhbrd/loan_default_app


Add AWS Access Credentials into github secrets . 

Now pipeline will work perfectly . 


###### How to add Apache Airflow .

ğŸ“Œ What Will Apache Airflow Do in This Project?
Apache Airflow will help you: 
âœ… Automate the CI/CD pipeline execution
âœ… Monitor and schedule Docker container deployment
âœ… Trigger workflows when new code is pushed
âœ… Retry failed steps and alert you in case of errors
âœ… Orchestrate multiple workflows (like data processing, ETL, ML models, etc.)


ğŸ›  Step 1: Install Docker on Your EC2 Instance
Since Airflow runs in Docker, install Docker on your EC2 instance:

sudo apt update
sudo apt install docker.io -y
sudo systemctl start docker
sudo systemctl enable docker

docker --version


ğŸ›  Step 2: Clone Your CI/CD Project in EC2

git clone https://github.com/your-repo.git
cd your-repo

ğŸ›  Step 3: Install Docker-Compose
Airflow requires docker-compose.

sudo apt install docker-compose -y


docker-compose --version



ğŸ›  Step 4: Create a docker-compose.yaml for Airflow
Inside your project directory, create an airflow folder:

mkdir airflow && cd airflow

Create a docker-compose.yaml file:
vim docker-compose.yaml

Add the following content:

version: '3.8'

services:
  airflow:
    image: apache/airflow:2.7.0
    container_name: airflow
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__WEBSERVER__SECRET_KEY=my_secret_key
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags


ğŸ›  Step 5: Start Airflow
Run the following command inside the airflow folder:

docker-compose up -d

Check running containers:
docker ps 

After it starts, open Airflow in a browser:
http://your-ec2-ip:8080

(Default username: airflow, password: airflow)


ğŸ›  Step 6: Create an Airflow DAG to Automate CI/CD
A DAG (Directed Acyclic Graph) in Airflow defines a workflow.

Create a dags folder:
mkdir dags && cd dags

Create a new DAG:
vim ci_cd_pipeline.py

Add the following code:

from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 3, 10),
    'retries': 1,
}

dag = DAG(
    'ci_cd_pipeline',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
)

build = BashOperator(
    task_id='build_image',
    bash_command='docker build -t myrepo/loan_default_app:latest .',
    dag=dag,
)

push = BashOperator(
    task_id='push_image',
    bash_command='docker push myrepo/loan_default_app:latest',
    dag=dag,
)

deploy = BashOperator(
    task_id='deploy_container',
    bash_command='''ssh -o StrictHostKeyChecking=no -i private_key.pem ubuntu@your-ec2-ip "
        docker pull myrepo/loan_default_app:latest &&
        docker stop loan_default_app || true &&
        docker rm loan_default_app || true &&
        docker run -d -p 8501:8501 --name loan_default_app myrepo/loan_default_app:latest"''',
    dag=dag,
)

build >> push >> deploy


ğŸ›  Step 7: Restart Airflow and Trigger DAG
Restart Airflow:

docker-compose down
docker-compose up -d


Go to the Airflow UI (http://your-ec2-ip:8080), enable ci_cd_pipeline DAG, and manually trigger it.

ğŸ“Œ Final Workflow
âœ… GitHub Actions triggers CI/CD on push
âœ… Apache Airflow schedules and monitors deployment
âœ… Airflow retries failures and alerts you





ğŸ”¹ Step 1: Open Apache Airflow UI
1ï¸âƒ£ Open your browser and go to:
http://your-ec2-ip:8080

ğŸ”¹ Replace your-ec2-ip with your actual EC2 public IP
ğŸ”¹ Example: If your EC2 IP is 3.92.145.27, go to:
http://3.92.145.27:8080

2ï¸âƒ£ Login to Airflow

Username: airflow
Password: airflow
(Unless you changed these in the docker-compose.yaml file)
ğŸ”¹ Step 2: Enable the ci_cd_pipeline DAG
1ï¸âƒ£ On the Airflow homepage, look for the DAGs list

You should see ci_cd_pipeline in the list.
2ï¸âƒ£ Enable the DAG:

Click the toggle switch in the "On/Off" column to turn it ON
âœ… This makes sure the DAG is active and can be triggered.
ğŸ”¹ Step 3: Manually Trigger the DAG
1ï¸âƒ£ Click on the ci_cd_pipeline DAG name to open it.
2ï¸âƒ£ Click the Trigger DAG â–¶ï¸ button (a play button in the top-right).
3ï¸âƒ£ This will start the CI/CD pipeline execution manually.

ğŸ¯ What Happens After Triggering?
âœ… Airflow will execute the DAG steps in order:

Build Docker Image
Push Image to Docker Hub
Deploy the container to AWS EC2
âœ… You can monitor logs in Airflow UI:

Click on a task (e.g., build_image)
Click "Logs" to see real-time execution output.

If you want it to run automatically, set schedule_interval='@daily' or a custom cron schedule.




